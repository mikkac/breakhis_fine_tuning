{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreakHis Image Classification with ðŸ¤— Vision Transformers and `TensorFlow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick intro: Vision Transformer (ViT) by Google Brain\n",
    "The Vision Transformer (ViT) is basically BERT, but applied to images. It attains excellent results compared to state-of-the-art convolutional networks. In order to provide images to the model, each image is split into a sequence of fixed-size patches (typically of resolution 16x16 or 32x32), which are linearly embedded. One also adds a [CLS] token at the beginning of the sequence in order to classify images. Next, one adds absolute position embeddings and provides this sequence to the Transformer encoder.\n",
    "\n",
    "* [Original paper](https://arxiv.org/abs/2010.11929)\n",
    "* [Official repo (in JAX)](https://github.com/google-research/vision_transformer)\n",
    "* [ðŸ¤— Vision Transformer](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "* [Pre-trained model](https://huggingface.co/google/vit-base-patch16-224-in21k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets \"tensorflow==2.6.0\" tensorflow-addons --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step, we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning process, e.g. `feature extractor` and `model` we will use. \n",
    "\n",
    "In this example we are going to fine-tune the [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) a Vision Transformer (ViT) pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.\n",
    "There are also [large](https://huggingface.co/google/vit-large-patch16-224-in21k) and [huge](https://huggingface.co/google/vit-huge-patch14-224-in21k) flavors of original ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "zoom = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:50:56.260949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 21:50:56.976746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64::/home/miki/anaconda3/lib/:/home/miki/anaconda3/envs/tf/lib/\n",
      "2023-05-10 21:50:56.976812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64::/home/miki/anaconda3/lib/:/home/miki/anaconda3/envs/tf/lib/\n",
      "2023-05-10 21:50:56.976821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import create_optimizer, DefaultDataCollator, ViTImageProcessor, TFViTForImageClassification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Pre-processing\n",
    "\n",
    "- **Data Source:** https://www.kaggle.com/code/nasrulhakim86/breast-cancer-histopathology-images-classification/data\n",
    "- The Breast Cancer Histopathological Image Classification (BreakHis) is composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients.\n",
    "- The images are collected using different magnifying factors (40X, 100X, 200X, and 400X). \n",
    "- To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format).\n",
    "- This database has been built in collaboration with the P&D Laboratory â€“ Pathological Anatomy and Cytopathology, Parana, Brazil (http://www.prevencaoediagnose.com.br). \n",
    "- Each image filename stores information about the image itself: method of procedure biopsy, tumor class, tumor type, patient identification, and magnification factor. \n",
    "- For example, SOBBTA-14-4659-40-001.png is the image 1, at magnification factor 40X, of a benign tumor of type tubular adenoma, original from the slide 14-4659, which was collected by procedure SOB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BreakHis` is not yet available as a dataset in the `datasets` library. To be able to create a `Dataset` instance we need to write a small little helper function, which will load our `Dataset` from the filesystem and create the instance to use later for training.\n",
    "\n",
    "This notebook assumes that the dataset is available in directory tree next to this file and its directory name is `breakhis_400x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/miki/Documents/studia/praca_dyplomowa/vcs/results/400x_2023_05_10-21_50_58')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "cwd = Path().absolute()\n",
    "input_path = cwd / f'breakhis_{zoom}x'\n",
    "output_path = cwd / 'results' / f'{zoom}x_{now}'\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_path, ignore_errors=True)\n",
    "os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of samples per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(train_val_csv)\n",
    "# group_counts = data.groupby('patient_id').size().reset_index(name='count')\n",
    "\n",
    "# group_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model using `Keras`\n",
    "\n",
    "Now that our `dataset` is processed, we can download the pretrained model and fine-tune it. But before we can do this we need to convert our Hugging Face `datasets` Dataset into a `tf.data.Dataset`. For this, we will use the `.to_tf_dataset` method and a `data collator` (Data collators are objects that will form a batch by using a list of dataset elements as input).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Laptop GPU, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:50:58.471626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:50:58.475669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:50:58.475859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:50:58.476477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "id2label = {\"0\": \"benign\", \"1\": \"malignant\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_train_epochs = 10\n",
    "train_batch_size = 3\n",
    "eval_batch_size = 3\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "output_dir = model_id.split(\"/\")[1]\n",
    "fp16 = True\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "if fp16:\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained transformer model and fine-tune it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a48793109f43f3984c20972dccb045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c7f948f062c03bd6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['file_loc', 'label', 'patient_id', 'pixel_values'],\n",
      "        num_rows: 1427\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.disable_traceback_filtering()\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_id)\n",
    "\n",
    "train_val_csv = str(input_path / \"train_val.csv\")\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_val_csv})\n",
    "\n",
    "\n",
    "def load_images(file_locs):\n",
    "    return [Image.open(file_loc).convert(\"RGB\") for file_loc in file_locs]\n",
    "\n",
    "\n",
    "images = load_images(dataset['train']['file_loc'])\n",
    "\n",
    "\n",
    "def process_example(image):\n",
    "    inputs = image_processor(image, return_tensors='tf')\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "\n",
    "def process_dataset(example, idx):\n",
    "    example['pixel_values'] = process_example(images[idx])\n",
    "\n",
    "    example['label'] = to_categorical(example['label'], num_classes=2)\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(process_dataset, with_indices=True, num_proc=1)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss():\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def get_metrics():\n",
    "    return [\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.AUC(name='auc', from_logits=True),\n",
    "        tf.keras.metrics.AUC(name='auc_multi', from_logits=True,\n",
    "                             num_labels=2, multi_label=True),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5),\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_callbacks(idx):\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "        CSVLogger(output_path / f'train_metrics_{idx}.csv')\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_optimizer(learning_rate, weight_decay_rate, num_warmup_steps, num_train_steps):\n",
    "    optimizer, _ = create_optimizer(\n",
    "        init_lr=learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        weight_decay_rate=weight_decay_rate,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "num_train_steps_list = []\n",
    "def train_model(idx, train, val):\n",
    "    num_train_steps = len(train) * num_train_epochs\n",
    "    num_train_steps_list.append(num_train_steps)\n",
    "    print(f\"num_train_steps = {num_train_steps}\")\n",
    "    optimizer = get_optimizer(\n",
    "        learning_rate, weight_decay_rate, num_warmup_steps, num_train_steps)\n",
    "\n",
    "    # load pre-trained ViT model\n",
    "    model = TFViTForImageClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=optimizer, loss=get_loss(), metrics=get_metrics())\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        callbacks=get_callbacks(idx),\n",
    "        epochs=num_train_epochs,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "train_val_data = dataset['train']\n",
    "files = np.array(train_val_data['file_loc'])\n",
    "labels = np.array(train_val_data['label'])\n",
    "patient_ids = np.array(train_val_data['patient_id'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom `StratifiedGroupKFold` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def map_nested_indices(nested_indices, original_indices):\n",
    "    return original_indices[nested_indices]\n",
    "\n",
    "class StratifiedGroupKFold:\n",
    "    def __init__(self, n_splits=5, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.used_group_ids = []\n",
    "\n",
    "    def _fill_bucket(self, bucket, class_counts, group_ids, y):\n",
    "        for group_id, label in zip(group_ids, y):\n",
    "            if group_id in self.used_group_ids:\n",
    "                continue\n",
    "            if class_counts[label] > 0:\n",
    "                group_indices = np.where(group_ids == group_id)[0]\n",
    "                bucket[label].extend(group_indices)\n",
    "                class_counts[label] -= len(group_indices)\n",
    "                self.used_group_ids.append(group_id)\n",
    "\n",
    "    def _create_buckets(self, group_ids, y, class_ratios):\n",
    "        total_samples = len(group_ids)\n",
    "        samples_per_split = total_samples // self.n_splits\n",
    "\n",
    "        buckets = []\n",
    "        for _ in range(self.n_splits):\n",
    "            bucket = {label: [] for label in class_ratios.keys()}\n",
    "            class_counts = {label: int(samples_per_split * ratio)\n",
    "                            for label, ratio in class_ratios.items()}\n",
    "            self._fill_bucket(bucket, class_counts, group_ids, y)\n",
    "            buckets.append(bucket)\n",
    "\n",
    "        return buckets\n",
    "\n",
    "    def _rotate_buckets(self, buckets):\n",
    "        return buckets[-1:] + buckets[:-1]\n",
    "\n",
    "    def _get_indices(self, bucket, group_ids, y):\n",
    "        indices = []\n",
    "        for label, groups in bucket.items():\n",
    "            for group in groups:\n",
    "                group_indices = np.where(group_ids == group)[0]\n",
    "                label_indices = np.where(y == label)[0]\n",
    "                indices.extend(np.intersect1d(group_indices, label_indices))\n",
    "        return np.array(indices)\n",
    "\n",
    "    def split(self, X, y, group_ids):\n",
    "        index_map = np.arange(len(y))\n",
    "        group_ids_s, y_s, index_map = shuffle(\n",
    "            group_ids, y, index_map, random_state=self.random_state)\n",
    "\n",
    "        class_ratios = {label: count / len(y)\n",
    "                        for label, count in Counter(y).items()}\n",
    "        buckets = self._create_buckets(group_ids_s, y_s, class_ratios)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            train_buckets = buckets[1:]\n",
    "            test_bucket = buckets[0]\n",
    "\n",
    "            train_indices = np.concatenate(\n",
    "                [np.array(bucket[label]) for bucket in train_buckets for label in bucket])\n",
    "            test_indices = np.concatenate(\n",
    "                [np.array(test_bucket[label]) for label in test_bucket])\n",
    "\n",
    "            # Map shuffled indices to original ones\n",
    "            train_indices = index_map[train_indices]\n",
    "            test_indices = index_map[test_indices]\n",
    "            \n",
    "            assert len(np.intersect1d(np.unique(group_ids[train_indices]), np.unique(group_ids[test_indices]))) == 0\n",
    "\n",
    "\n",
    "            yield train_indices, test_indices\n",
    "            buckets = self._rotate_buckets(buckets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `StratifiedGroupKFold` usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "# # Wygeneruj dane\n",
    "# n_samples = 1800\n",
    "# n_features = 2\n",
    "\n",
    "# X = np.random.rand(n_samples, n_features)\n",
    "# y = np.random.randint(0, 2, n_samples)\n",
    "# group_ids = np.repeat(np.arange(n_samples // 4), 4)\n",
    "\n",
    "# # Mieszamy dane, etykiety i grupy\n",
    "# X, y, group_ids = shuffle(X, y, group_ids, random_state=42)\n",
    "\n",
    "# # UtwÃ³rz instancjÄ™ StratifiedGroupKFold\n",
    "# skf = StratifiedGroupKFold(n_splits=3, random_state=42)\n",
    "\n",
    "# # Iteruj przez foldy\n",
    "# for train_index, test_index in skf.split(X, y, group_ids):\n",
    "#     print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "#     # X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     train_groups, test_groups = group_ids[train_index], group_ids[test_index]\n",
    "#     train_class_ratio = {label: count / len(y_train) for label, count in Counter(y_train).items()}\n",
    "#     print(f\"Train class ratio: {train_class_ratio}\")\n",
    "#     test_class_ratio = {label: count / len(y_test) for label, count in Counter(y_test).items()}\n",
    "#     print(f\"test class ratio: {test_class_ratio}\")\n",
    "#     assert len(np.intersect1d(np.unique(train_groups), np.unique(test_groups))) == 0\n",
    "#     print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_val_indices(idx, indices):\n",
    "    return idx in indices\n",
    "\n",
    "def remove_extra_dim(example):\n",
    "    example['pixel_values'] = np.squeeze(example['pixel_values'], axis=0)\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "sgfk = StratifiedGroupKFold(n_splits=n_splits, random_state=42)\n",
    "\n",
    "folds = sgfk.split(files, np.argmax(labels, axis=1), patient_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_histories = []\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "def run_fold(idx):\n",
    "    (train_index, val_index) = next(folds)\n",
    "\n",
    "    # train_index = map_nested_indices(train_index, train_val_index)\n",
    "    # val_index = map_nested_indices(val_index, train_val_index)\n",
    "\n",
    "    # Check indices uniqueness\n",
    "    train_index_unique, train_counts = np.unique(\n",
    "        train_index, return_counts=True)\n",
    "    val_index_unique, val_counts = np.unique(val_index, return_counts=True)\n",
    "    are_all_values_unique = np.all(\n",
    "        train_counts == 1) and np.all(val_counts == 1)\n",
    "\n",
    "    print('Are all indices', are_all_values_unique)\n",
    "    print(\n",
    "        f'Indices shared between train & val splits (should be empty): {intersection(train_index_unique, val_index_unique)}')\n",
    "\n",
    "    # Check patient ids uniqueness\n",
    "    train_data_filtered = train_val_data.filter(lambda _, idx: filter_train_val_indices(\n",
    "        idx, train_index), with_indices=True).map(remove_extra_dim)\n",
    "    val_data_filtered = train_val_data.filter(lambda _, idx: filter_train_val_indices(\n",
    "        idx, val_index), with_indices=True).map(remove_extra_dim)\n",
    "\n",
    "    train_ids_unique = np.unique(train_data_filtered['patient_id'])\n",
    "    val_ids_unique = np.unique(val_data_filtered['patient_id'])\n",
    "\n",
    "    print(f'Train patient IDs: {len(train_ids_unique)}')\n",
    "    print(f'Val patient IDs: {len(val_ids_unique)}')\n",
    "    print(\n",
    "        f'Train + Val patient IDs: {len(train_ids_unique) + len(val_ids_unique)}')\n",
    "    print(\n",
    "        f'Patient IDs shared between train & val splits (should be empty): {intersection(train_ids_unique, val_ids_unique)}')\n",
    "\n",
    "    print(f'Train patient ids: {train_ids_unique}')\n",
    "    print(f'Val patient ids: {val_ids_unique}')\n",
    "    assert len(np.intersect1d(train_ids_unique, val_ids_unique)) == 0\n",
    "\n",
    "    # Create datasets and train model\n",
    "    train_dataset = train_data_filtered.to_tf_dataset(\n",
    "        columns=['pixel_values'],\n",
    "        label_cols=['label'],\n",
    "        shuffle=True,\n",
    "        batch_size=train_batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    val_dataset = val_data_filtered.to_tf_dataset(\n",
    "        columns=['pixel_values'],\n",
    "        label_cols=['label'],\n",
    "        shuffle=True,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    print(train_dataset)\n",
    "    print(val_dataset)\n",
    "\n",
    "    # model_with_history = {}\n",
    "    model_with_history = train_model(idx, train_dataset, val_dataset)\n",
    "    model_with_history['patient_ids'] = {\n",
    "        'train': list(train_ids_unique), 'val': list(val_ids_unique)}\n",
    "\n",
    "    models_with_histories.append(model_with_history)\n",
    "\n",
    "    print(f'Fold {idx + 1}/{n_splits} finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bd94e691d43e79b1.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a54dfef17c92cdf9.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7db0b6ffe2457e9d.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2637fd494c64f9d0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all indices True\n",
      "Indices shared between train & val splits (should be empty): []\n",
      "Train patient IDs: 54\n",
      "Val patient IDs: 13\n",
      "Train + Val patient IDs: 67\n",
      "Patient IDs shared between train & val splits (should be empty): []\n",
      "Train patient ids: ['SOB_B_A-14-22549G' 'SOB_B_A-14-29960CD' 'SOB_B_F-14-14134'\n",
      " 'SOB_B_F-14-14134E' 'SOB_B_F-14-21998CD' 'SOB_B_F-14-23060CD'\n",
      " 'SOB_B_F-14-23222AB' 'SOB_B_F-14-25197' 'SOB_B_PT-14-22704'\n",
      " 'SOB_B_PT-14-29315EF' 'SOB_B_TA-14-13200' 'SOB_B_TA-14-15275'\n",
      " 'SOB_B_TA-14-16184' 'SOB_B_TA-14-19854C' 'SOB_B_TA-14-21978AB'\n",
      " 'SOB_B_TA-14-3411F' 'SOB_M_DC-14-10926' 'SOB_M_DC-14-11031'\n",
      " 'SOB_M_DC-14-11951' 'SOB_M_DC-14-13412' 'SOB_M_DC-14-14015'\n",
      " 'SOB_M_DC-14-14926' 'SOB_M_DC-14-14946' 'SOB_M_DC-14-15572'\n",
      " 'SOB_M_DC-14-15696' 'SOB_M_DC-14-16188' 'SOB_M_DC-14-16336'\n",
      " 'SOB_M_DC-14-16716' 'SOB_M_DC-14-17614' 'SOB_M_DC-14-17915'\n",
      " 'SOB_M_DC-14-18650' 'SOB_M_DC-14-2523' 'SOB_M_DC-14-2773'\n",
      " 'SOB_M_DC-14-2980' 'SOB_M_DC-14-3909' 'SOB_M_DC-14-4364'\n",
      " 'SOB_M_DC-14-4372' 'SOB_M_DC-14-5694' 'SOB_M_DC-14-5695'\n",
      " 'SOB_M_DC-14-6241' 'SOB_M_DC-14-8168' 'SOB_M_LC-14-12204'\n",
      " 'SOB_M_LC-14-13412' 'SOB_M_LC-14-15570' 'SOB_M_MC-14-10147'\n",
      " 'SOB_M_MC-14-13418DE' 'SOB_M_MC-14-18842' 'SOB_M_MC-14-18842D'\n",
      " 'SOB_M_PC-14-12465' 'SOB_M_PC-14-15687B' 'SOB_M_PC-14-15704'\n",
      " 'SOB_M_PC-14-19440' 'SOB_M_PC-14-9146' 'SOB_M_PC-15-190EF']\n",
      "Val patient ids: ['SOB_B_A-14-22549AB' 'SOB_B_F-14-23060AB' 'SOB_B_F-14-29960AB'\n",
      " 'SOB_B_F-14-9133' 'SOB_M_DC-14-12312' 'SOB_M_DC-14-15792'\n",
      " 'SOB_M_DC-14-16875' 'SOB_M_DC-14-20629' 'SOB_M_DC-14-20636'\n",
      " 'SOB_M_LC-14-15570C' 'SOB_M_LC-14-16196' 'SOB_M_MC-14-13413'\n",
      " 'SOB_M_MC-14-19979']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:51:12.667269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 21:51:12.668079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:12.668439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:12.668705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:13.109771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:13.109951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:13.110092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-10 21:51:13.110193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6056 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "num_train_steps = 3780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:51:15.224728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-05-10 21:51:15.801505: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-10 21:51:15.802465: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-10 21:51:15.802484: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:85] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-05-10 21:51:15.803432: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-10 21:51:15.803491: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/miki/.local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "378/378 [==============================] - 48s 86ms/step - loss: 0.3864 - accuracy: 0.8288 - auc: 0.9236 - auc_multi: 0.9100 - recall: 0.7440 - precision: 0.8959 - f1_score: 0.7454 - val_loss: 0.2727 - val_accuracy: 0.9065 - val_auc: 0.9642 - val_auc_multi: 0.9688 - val_recall: 0.8980 - val_precision: 0.9135 - val_f1_score: 0.8894\n",
      "Epoch 2/10\n",
      "378/378 [==============================] - 30s 80ms/step - loss: 0.1939 - accuracy: 0.9448 - auc: 0.9826 - auc_multi: 0.9795 - recall: 0.9276 - precision: 0.9607 - f1_score: 0.9326 - val_loss: 0.3367 - val_accuracy: 0.8759 - val_auc: 0.9383 - val_auc_multi: 0.9562 - val_recall: 0.8639 - val_precision: 0.8850 - val_f1_score: 0.8216\n",
      "Epoch 3/10\n",
      "378/378 [==============================] - 30s 80ms/step - loss: 0.1252 - accuracy: 0.9669 - auc: 0.9940 - auc_multi: 0.9929 - recall: 0.9612 - precision: 0.9723 - f1_score: 0.9611 - val_loss: 0.2123 - val_accuracy: 0.9286 - val_auc: 0.9816 - val_auc_multi: 0.9840 - val_recall: 0.9252 - val_precision: 0.9315 - val_f1_score: 0.9192\n",
      "Epoch 4/10\n",
      "378/378 [==============================] - 30s 80ms/step - loss: 0.0976 - accuracy: 0.9709 - auc: 0.9967 - auc_multi: 0.9963 - recall: 0.9638 - precision: 0.9776 - f1_score: 0.9674 - val_loss: 0.1938 - val_accuracy: 0.9337 - val_auc: 0.9777 - val_auc_multi: 0.9781 - val_recall: 0.9252 - val_precision: 0.9412 - val_f1_score: 0.9196\n",
      "Epoch 5/10\n",
      "378/378 [==============================] - 30s 80ms/step - loss: 0.0458 - accuracy: 0.9938 - auc: 0.9998 - auc_multi: 0.9997 - recall: 0.9929 - precision: 0.9947 - f1_score: 0.9924 - val_loss: 0.1603 - val_accuracy: 0.9439 - val_auc: 0.9784 - val_auc_multi: 0.9738 - val_recall: 0.9320 - val_precision: 0.9547 - val_f1_score: 0.9361\n",
      "Epoch 6/10\n",
      "378/378 [==============================] - 30s 80ms/step - loss: 0.0366 - accuracy: 0.9943 - auc: 0.9996 - auc_multi: 0.9993 - recall: 0.9938 - precision: 0.9947 - f1_score: 0.9931 - val_loss: 0.1899 - val_accuracy: 0.9456 - val_auc: 0.9705 - val_auc_multi: 0.9660 - val_recall: 0.9422 - val_precision: 0.9486 - val_f1_score: 0.9352\n",
      "Epoch 7/10\n",
      "378/378 [==============================] - 30s 81ms/step - loss: 0.0183 - accuracy: 1.0000 - auc: 1.0000 - auc_multi: 1.0000 - recall: 1.0000 - precision: 1.0000 - f1_score: 1.0000 - val_loss: 0.2062 - val_accuracy: 0.9456 - val_auc: 0.9734 - val_auc_multi: 0.9722 - val_recall: 0.9422 - val_precision: 0.9486 - val_f1_score: 0.9348\n",
      "Epoch 8/10\n",
      "378/378 [==============================] - 30s 81ms/step - loss: 0.0154 - accuracy: 1.0000 - auc: 1.0000 - auc_multi: 1.0000 - recall: 1.0000 - precision: 1.0000 - f1_score: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9456 - val_auc: 0.9611 - val_auc_multi: 0.9509 - val_recall: 0.9422 - val_precision: 0.9486 - val_f1_score: 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-672ce7abea409518.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4a1ce6bbfdc5aa01.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-edf5c95ddaed5554.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c44d1f68b06d3122.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 finished\n",
      "Are all indices True\n",
      "Indices shared between train & val splits (should be empty): []\n",
      "Train patient IDs: 53\n",
      "Val patient IDs: 14\n",
      "Train + Val patient IDs: 67\n",
      "Patient IDs shared between train & val splits (should be empty): []\n",
      "Train patient ids: ['SOB_B_A-14-22549AB' 'SOB_B_A-14-22549G' 'SOB_B_A-14-29960CD'\n",
      " 'SOB_B_F-14-14134' 'SOB_B_F-14-21998CD' 'SOB_B_F-14-23060AB'\n",
      " 'SOB_B_F-14-23060CD' 'SOB_B_F-14-23222AB' 'SOB_B_F-14-25197'\n",
      " 'SOB_B_F-14-29960AB' 'SOB_B_F-14-9133' 'SOB_B_PT-14-22704'\n",
      " 'SOB_B_TA-14-13200' 'SOB_B_TA-14-15275' 'SOB_B_TA-14-16184'\n",
      " 'SOB_B_TA-14-19854C' 'SOB_B_TA-14-3411F' 'SOB_M_DC-14-12312'\n",
      " 'SOB_M_DC-14-13412' 'SOB_M_DC-14-14015' 'SOB_M_DC-14-14926'\n",
      " 'SOB_M_DC-14-14946' 'SOB_M_DC-14-15792' 'SOB_M_DC-14-16188'\n",
      " 'SOB_M_DC-14-16716' 'SOB_M_DC-14-16875' 'SOB_M_DC-14-17614'\n",
      " 'SOB_M_DC-14-17915' 'SOB_M_DC-14-18650' 'SOB_M_DC-14-20629'\n",
      " 'SOB_M_DC-14-20636' 'SOB_M_DC-14-2523' 'SOB_M_DC-14-2773'\n",
      " 'SOB_M_DC-14-2980' 'SOB_M_DC-14-3909' 'SOB_M_DC-14-4372'\n",
      " 'SOB_M_DC-14-5694' 'SOB_M_DC-14-8168' 'SOB_M_LC-14-12204'\n",
      " 'SOB_M_LC-14-13412' 'SOB_M_LC-14-15570' 'SOB_M_LC-14-15570C'\n",
      " 'SOB_M_LC-14-16196' 'SOB_M_MC-14-10147' 'SOB_M_MC-14-13413'\n",
      " 'SOB_M_MC-14-18842' 'SOB_M_MC-14-19979' 'SOB_M_PC-14-12465'\n",
      " 'SOB_M_PC-14-15687B' 'SOB_M_PC-14-15704' 'SOB_M_PC-14-19440'\n",
      " 'SOB_M_PC-14-9146' 'SOB_M_PC-15-190EF']\n",
      "Val patient ids: ['SOB_B_F-14-14134E' 'SOB_B_PT-14-29315EF' 'SOB_B_TA-14-21978AB'\n",
      " 'SOB_M_DC-14-10926' 'SOB_M_DC-14-11031' 'SOB_M_DC-14-11951'\n",
      " 'SOB_M_DC-14-15572' 'SOB_M_DC-14-15696' 'SOB_M_DC-14-16336'\n",
      " 'SOB_M_DC-14-4364' 'SOB_M_DC-14-5695' 'SOB_M_DC-14-6241'\n",
      " 'SOB_M_MC-14-13418DE' 'SOB_M_MC-14-18842D']\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "num_train_steps = 4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "410/410 [==============================] - 47s 82ms/step - loss: 0.3885 - accuracy: 0.8397 - auc: 0.9181 - auc_multi: 0.9010 - recall: 0.7608 - precision: 0.9034 - f1_score: 0.7844 - val_loss: 0.3364 - val_accuracy: 0.8283 - val_auc: 0.9375 - val_auc_multi: 0.9415 - val_recall: 0.6717 - val_precision: 0.9779 - val_f1_score: 0.4779\n",
      "Epoch 2/10\n",
      "410/410 [==============================] - 32s 77ms/step - loss: 0.1959 - accuracy: 0.9378 - auc: 0.9833 - auc_multi: 0.9793 - recall: 0.9203 - precision: 0.9536 - f1_score: 0.9248 - val_loss: 0.2266 - val_accuracy: 0.8889 - val_auc: 0.9793 - val_auc_multi: 0.9820 - val_recall: 0.8687 - val_precision: 0.9053 - val_f1_score: 0.8583\n",
      "Epoch 3/10\n",
      "410/410 [==============================] - 31s 77ms/step - loss: 0.0920 - accuracy: 0.9809 - auc: 0.9965 - auc_multi: 0.9957 - recall: 0.9740 - precision: 0.9876 - f1_score: 0.9785 - val_loss: 0.4309 - val_accuracy: 0.8384 - val_auc: 0.9391 - val_auc_multi: 0.9105 - val_recall: 0.8283 - val_precision: 0.8454 - val_f1_score: 0.7343\n",
      "Epoch 4/10\n",
      "410/410 [==============================] - 32s 77ms/step - loss: 0.0667 - accuracy: 0.9858 - auc: 0.9980 - auc_multi: 0.9976 - recall: 0.9821 - precision: 0.9893 - f1_score: 0.9841 - val_loss: 0.2825 - val_accuracy: 0.8990 - val_auc: 0.9704 - val_auc_multi: 0.9537 - val_recall: 0.8838 - val_precision: 0.9115 - val_f1_score: 0.8502\n",
      "Epoch 5/10\n",
      "410/410 [==============================] - 32s 78ms/step - loss: 0.0301 - accuracy: 0.9980 - auc: 1.0000 - auc_multi: 1.0000 - recall: 0.9967 - precision: 0.9992 - f1_score: 0.9978 - val_loss: 0.5426 - val_accuracy: 0.8283 - val_auc: 0.9286 - val_auc_multi: 0.8856 - val_recall: 0.8131 - val_precision: 0.8385 - val_f1_score: 0.7122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-09429a8004943140.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-467e29293f6c934c.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-900177109df0c039.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7bd3dbfa8d84ec5b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5 finished\n",
      "Are all indices True\n",
      "Indices shared between train & val splits (should be empty): []\n",
      "Train patient IDs: 51\n",
      "Val patient IDs: 16\n",
      "Train + Val patient IDs: 67\n",
      "Patient IDs shared between train & val splits (should be empty): []\n",
      "Train patient ids: ['SOB_B_A-14-22549AB' 'SOB_B_A-14-22549G' 'SOB_B_A-14-29960CD'\n",
      " 'SOB_B_F-14-14134' 'SOB_B_F-14-14134E' 'SOB_B_F-14-23060AB'\n",
      " 'SOB_B_F-14-25197' 'SOB_B_F-14-29960AB' 'SOB_B_F-14-9133'\n",
      " 'SOB_B_PT-14-22704' 'SOB_B_PT-14-29315EF' 'SOB_B_TA-14-13200'\n",
      " 'SOB_B_TA-14-15275' 'SOB_B_TA-14-21978AB' 'SOB_B_TA-14-3411F'\n",
      " 'SOB_M_DC-14-10926' 'SOB_M_DC-14-11031' 'SOB_M_DC-14-11951'\n",
      " 'SOB_M_DC-14-12312' 'SOB_M_DC-14-14946' 'SOB_M_DC-14-15572'\n",
      " 'SOB_M_DC-14-15696' 'SOB_M_DC-14-15792' 'SOB_M_DC-14-16188'\n",
      " 'SOB_M_DC-14-16336' 'SOB_M_DC-14-16716' 'SOB_M_DC-14-16875'\n",
      " 'SOB_M_DC-14-17614' 'SOB_M_DC-14-20629' 'SOB_M_DC-14-20636'\n",
      " 'SOB_M_DC-14-2523' 'SOB_M_DC-14-2773' 'SOB_M_DC-14-4364'\n",
      " 'SOB_M_DC-14-4372' 'SOB_M_DC-14-5695' 'SOB_M_DC-14-6241'\n",
      " 'SOB_M_DC-14-8168' 'SOB_M_LC-14-12204' 'SOB_M_LC-14-13412'\n",
      " 'SOB_M_LC-14-15570' 'SOB_M_LC-14-15570C' 'SOB_M_LC-14-16196'\n",
      " 'SOB_M_MC-14-13413' 'SOB_M_MC-14-13418DE' 'SOB_M_MC-14-18842D'\n",
      " 'SOB_M_MC-14-19979' 'SOB_M_PC-14-12465' 'SOB_M_PC-14-15687B'\n",
      " 'SOB_M_PC-14-15704' 'SOB_M_PC-14-19440' 'SOB_M_PC-14-9146']\n",
      "Val patient ids: ['SOB_B_F-14-21998CD' 'SOB_B_F-14-23060CD' 'SOB_B_F-14-23222AB'\n",
      " 'SOB_B_TA-14-16184' 'SOB_B_TA-14-19854C' 'SOB_M_DC-14-13412'\n",
      " 'SOB_M_DC-14-14015' 'SOB_M_DC-14-14926' 'SOB_M_DC-14-17915'\n",
      " 'SOB_M_DC-14-18650' 'SOB_M_DC-14-2980' 'SOB_M_DC-14-3909'\n",
      " 'SOB_M_DC-14-5694' 'SOB_M_MC-14-10147' 'SOB_M_MC-14-18842'\n",
      " 'SOB_M_PC-15-190EF']\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "num_train_steps = 3730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "373/373 [==============================] - 47s 88ms/step - loss: 0.3546 - accuracy: 0.8487 - auc: 0.9379 - auc_multi: 0.9264 - recall: 0.7672 - precision: 0.9166 - f1_score: 0.7839 - val_loss: 0.3713 - val_accuracy: 0.8403 - val_auc: 0.9376 - val_auc_multi: 0.9386 - val_recall: 0.8194 - val_precision: 0.8552 - val_f1_score: 0.8348\n",
      "Epoch 2/10\n",
      "373/373 [==============================] - 31s 82ms/step - loss: 0.1937 - accuracy: 0.9436 - auc: 0.9774 - auc_multi: 0.9725 - recall: 0.9311 - precision: 0.9550 - f1_score: 0.9329 - val_loss: 0.6423 - val_accuracy: 0.7000 - val_auc: 0.8570 - val_auc_multi: 0.8577 - val_recall: 0.6839 - val_precision: 0.7067 - val_f1_score: 0.6147\n",
      "Epoch 3/10\n",
      "373/373 [==============================] - 31s 82ms/step - loss: 0.1012 - accuracy: 0.9785 - auc: 0.9923 - auc_multi: 0.9890 - recall: 0.9767 - precision: 0.9802 - f1_score: 0.9742 - val_loss: 0.4458 - val_accuracy: 0.8290 - val_auc: 0.9382 - val_auc_multi: 0.9230 - val_recall: 0.8226 - val_precision: 0.8333 - val_f1_score: 0.8147\n",
      "Epoch 4/10\n",
      "373/373 [==============================] - 31s 82ms/step - loss: 0.0688 - accuracy: 0.9852 - auc: 0.9978 - auc_multi: 0.9973 - recall: 0.9812 - precision: 0.9892 - f1_score: 0.9817 - val_loss: 0.4328 - val_accuracy: 0.8645 - val_auc: 0.9458 - val_auc_multi: 0.9281 - val_recall: 0.8613 - val_precision: 0.8669 - val_f1_score: 0.8597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0065557df797b3f7.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-05acabea464fe84e.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-70a14f2cb1668398.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b64585082c8b492.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5 finished\n",
      "Are all indices True\n",
      "Indices shared between train & val splits (should be empty): []\n",
      "Train patient IDs: 55\n",
      "Val patient IDs: 12\n",
      "Train + Val patient IDs: 67\n",
      "Patient IDs shared between train & val splits (should be empty): []\n",
      "Train patient ids: ['SOB_B_A-14-22549AB' 'SOB_B_A-14-22549G' 'SOB_B_A-14-29960CD'\n",
      " 'SOB_B_F-14-14134' 'SOB_B_F-14-14134E' 'SOB_B_F-14-21998CD'\n",
      " 'SOB_B_F-14-23060AB' 'SOB_B_F-14-23060CD' 'SOB_B_F-14-23222AB'\n",
      " 'SOB_B_F-14-29960AB' 'SOB_B_F-14-9133' 'SOB_B_PT-14-29315EF'\n",
      " 'SOB_B_TA-14-15275' 'SOB_B_TA-14-16184' 'SOB_B_TA-14-19854C'\n",
      " 'SOB_B_TA-14-21978AB' 'SOB_M_DC-14-10926' 'SOB_M_DC-14-11031'\n",
      " 'SOB_M_DC-14-11951' 'SOB_M_DC-14-12312' 'SOB_M_DC-14-13412'\n",
      " 'SOB_M_DC-14-14015' 'SOB_M_DC-14-14926' 'SOB_M_DC-14-15572'\n",
      " 'SOB_M_DC-14-15696' 'SOB_M_DC-14-15792' 'SOB_M_DC-14-16188'\n",
      " 'SOB_M_DC-14-16336' 'SOB_M_DC-14-16875' 'SOB_M_DC-14-17915'\n",
      " 'SOB_M_DC-14-18650' 'SOB_M_DC-14-20629' 'SOB_M_DC-14-20636'\n",
      " 'SOB_M_DC-14-2523' 'SOB_M_DC-14-2773' 'SOB_M_DC-14-2980'\n",
      " 'SOB_M_DC-14-3909' 'SOB_M_DC-14-4364' 'SOB_M_DC-14-4372'\n",
      " 'SOB_M_DC-14-5694' 'SOB_M_DC-14-5695' 'SOB_M_DC-14-6241'\n",
      " 'SOB_M_LC-14-15570' 'SOB_M_LC-14-15570C' 'SOB_M_LC-14-16196'\n",
      " 'SOB_M_MC-14-10147' 'SOB_M_MC-14-13413' 'SOB_M_MC-14-13418DE'\n",
      " 'SOB_M_MC-14-18842' 'SOB_M_MC-14-18842D' 'SOB_M_MC-14-19979'\n",
      " 'SOB_M_PC-14-12465' 'SOB_M_PC-14-19440' 'SOB_M_PC-14-9146'\n",
      " 'SOB_M_PC-15-190EF']\n",
      "Val patient ids: ['SOB_B_F-14-25197' 'SOB_B_PT-14-22704' 'SOB_B_TA-14-13200'\n",
      " 'SOB_B_TA-14-3411F' 'SOB_M_DC-14-14946' 'SOB_M_DC-14-16716'\n",
      " 'SOB_M_DC-14-17614' 'SOB_M_DC-14-8168' 'SOB_M_LC-14-12204'\n",
      " 'SOB_M_LC-14-13412' 'SOB_M_PC-14-15687B' 'SOB_M_PC-14-15704']\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "num_train_steps = 3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 47s 88ms/step - loss: 0.3156 - accuracy: 0.8767 - auc: 0.9503 - auc_multi: 0.9371 - recall: 0.8157 - precision: 0.9290 - f1_score: 0.8309 - val_loss: 0.5400 - val_accuracy: 0.7911 - val_auc: 0.7799 - val_auc_multi: 0.6918 - val_recall: 0.7632 - val_precision: 0.8084 - val_f1_score: 0.7243\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 31s 82ms/step - loss: 0.1399 - accuracy: 0.9671 - auc: 0.9904 - auc_multi: 0.9875 - recall: 0.9590 - precision: 0.9747 - f1_score: 0.9588 - val_loss: 0.5014 - val_accuracy: 0.8355 - val_auc: 0.8204 - val_auc_multi: 0.7550 - val_recall: 0.8289 - val_precision: 0.8400 - val_f1_score: 0.8067\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 31s 82ms/step - loss: 0.0839 - accuracy: 0.9800 - auc: 0.9985 - auc_multi: 0.9980 - recall: 0.9724 - precision: 0.9873 - f1_score: 0.9758 - val_loss: 0.5729 - val_accuracy: 0.8339 - val_auc: 0.8100 - val_auc_multi: 0.7445 - val_recall: 0.8289 - val_precision: 0.8372 - val_f1_score: 0.8079\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 31s 82ms/step - loss: 0.0470 - accuracy: 0.9938 - auc: 0.9988 - auc_multi: 0.9988 - recall: 0.9929 - precision: 0.9946 - f1_score: 0.9922 - val_loss: 0.7820 - val_accuracy: 0.7582 - val_auc: 0.7750 - val_auc_multi: 0.7343 - val_recall: 0.7401 - val_precision: 0.7679 - val_f1_score: 0.7360\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 31s 82ms/step - loss: 0.0239 - accuracy: 0.9991 - auc: 1.0000 - auc_multi: 1.0000 - recall: 0.9991 - precision: 0.9991 - f1_score: 0.9989 - val_loss: 0.6709 - val_accuracy: 0.8339 - val_auc: 0.8388 - val_auc_multi: 0.7938 - val_recall: 0.8322 - val_precision: 0.8350 - val_f1_score: 0.8053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25a1e5a04b697f60.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e6a67e99fc1bf625.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5dc1d6945bbb02f4.arrow\n",
      "Loading cached processed dataset at /home/miki/.cache/huggingface/datasets/csv/default-0010e7a0edeaefdb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b85136c4211e8e0e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5 finished\n",
      "Are all indices True\n",
      "Indices shared between train & val splits (should be empty): []\n",
      "Train patient IDs: 55\n",
      "Val patient IDs: 12\n",
      "Train + Val patient IDs: 67\n",
      "Patient IDs shared between train & val splits (should be empty): []\n",
      "Train patient ids: ['SOB_B_A-14-22549AB' 'SOB_B_F-14-14134E' 'SOB_B_F-14-21998CD'\n",
      " 'SOB_B_F-14-23060AB' 'SOB_B_F-14-23060CD' 'SOB_B_F-14-23222AB'\n",
      " 'SOB_B_F-14-25197' 'SOB_B_F-14-29960AB' 'SOB_B_F-14-9133'\n",
      " 'SOB_B_PT-14-22704' 'SOB_B_PT-14-29315EF' 'SOB_B_TA-14-13200'\n",
      " 'SOB_B_TA-14-16184' 'SOB_B_TA-14-19854C' 'SOB_B_TA-14-21978AB'\n",
      " 'SOB_B_TA-14-3411F' 'SOB_M_DC-14-10926' 'SOB_M_DC-14-11031'\n",
      " 'SOB_M_DC-14-11951' 'SOB_M_DC-14-12312' 'SOB_M_DC-14-13412'\n",
      " 'SOB_M_DC-14-14015' 'SOB_M_DC-14-14926' 'SOB_M_DC-14-14946'\n",
      " 'SOB_M_DC-14-15572' 'SOB_M_DC-14-15696' 'SOB_M_DC-14-15792'\n",
      " 'SOB_M_DC-14-16336' 'SOB_M_DC-14-16716' 'SOB_M_DC-14-16875'\n",
      " 'SOB_M_DC-14-17614' 'SOB_M_DC-14-17915' 'SOB_M_DC-14-18650'\n",
      " 'SOB_M_DC-14-20629' 'SOB_M_DC-14-20636' 'SOB_M_DC-14-2980'\n",
      " 'SOB_M_DC-14-3909' 'SOB_M_DC-14-4364' 'SOB_M_DC-14-5694'\n",
      " 'SOB_M_DC-14-5695' 'SOB_M_DC-14-6241' 'SOB_M_DC-14-8168'\n",
      " 'SOB_M_LC-14-12204' 'SOB_M_LC-14-13412' 'SOB_M_LC-14-15570C'\n",
      " 'SOB_M_LC-14-16196' 'SOB_M_MC-14-10147' 'SOB_M_MC-14-13413'\n",
      " 'SOB_M_MC-14-13418DE' 'SOB_M_MC-14-18842' 'SOB_M_MC-14-18842D'\n",
      " 'SOB_M_MC-14-19979' 'SOB_M_PC-14-15687B' 'SOB_M_PC-14-15704'\n",
      " 'SOB_M_PC-15-190EF']\n",
      "Val patient ids: ['SOB_B_A-14-22549G' 'SOB_B_A-14-29960CD' 'SOB_B_F-14-14134'\n",
      " 'SOB_B_TA-14-15275' 'SOB_M_DC-14-16188' 'SOB_M_DC-14-2523'\n",
      " 'SOB_M_DC-14-2773' 'SOB_M_DC-14-4372' 'SOB_M_LC-14-15570'\n",
      " 'SOB_M_PC-14-12465' 'SOB_M_PC-14-19440' 'SOB_M_PC-14-9146']\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "num_train_steps = 3690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "369/369 [==============================] - 47s 89ms/step - loss: 0.3288 - accuracy: 0.8657 - auc: 0.9499 - auc_multi: 0.9378 - recall: 0.7875 - precision: 0.9335 - f1_score: 0.8153 - val_loss: 0.2637 - val_accuracy: 0.9081 - val_auc: 0.9695 - val_auc_multi: 0.9703 - val_recall: 0.8972 - val_precision: 0.9172 - val_f1_score: 0.8848\n",
      "Epoch 2/10\n",
      "369/369 [==============================] - 31s 83ms/step - loss: 0.1522 - accuracy: 0.9561 - auc: 0.9911 - auc_multi: 0.9896 - recall: 0.9430 - precision: 0.9684 - f1_score: 0.9480 - val_loss: 0.3190 - val_accuracy: 0.8785 - val_auc: 0.9484 - val_auc_multi: 0.9458 - val_recall: 0.8629 - val_precision: 0.8907 - val_f1_score: 0.8293\n",
      "Epoch 3/10\n",
      "369/369 [==============================] - 31s 83ms/step - loss: 0.0616 - accuracy: 0.9914 - auc: 0.9997 - auc_multi: 0.9997 - recall: 0.9882 - precision: 0.9945 - f1_score: 0.9893 - val_loss: 0.2935 - val_accuracy: 0.9050 - val_auc: 0.9509 - val_auc_multi: 0.9397 - val_recall: 0.8972 - val_precision: 0.9114 - val_f1_score: 0.8785\n",
      "Epoch 4/10\n",
      "369/369 [==============================] - 31s 83ms/step - loss: 0.0286 - accuracy: 1.0000 - auc: 1.0000 - auc_multi: 1.0000 - recall: 1.0000 - precision: 1.0000 - f1_score: 1.0000 - val_loss: 0.3660 - val_accuracy: 0.8863 - val_auc: 0.9367 - val_auc_multi: 0.9213 - val_recall: 0.8847 - val_precision: 0.8875 - val_f1_score: 0.8546\n",
      "Fold 5/5 finished\n"
     ]
    }
   ],
   "source": [
    "for idx in range(n_splits):\n",
    "    run_fold(idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, model_with_history in enumerate(models_with_histories):\n",
    "#     history = model_with_history.get('history', None)\n",
    "#     np.save(output_path / f'train_history_{idx}.npy', history.history)\n",
    "\n",
    "# To load:\n",
    "# history = np.load(output_path / f'train_history_{idx}.npy', allow_pickle='TRUE').item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model index: 0, val_accuracy: 0.9455782175064088\n",
      "{\n",
      "    \"idx\": 0,\n",
      "    \"model_id\": \"google/vit-base-patch16-224-in21k\",\n",
      "    \"zoom\": 400,\n",
      "    \"n_splits\": 5,\n",
      "    \"num_train_epochs\": 10,\n",
      "    \"train_batch_size\": 3,\n",
      "    \"eval_batch_size\": 3,\n",
      "    \"learning_rate\": 3e-05,\n",
      "    \"weight_decay_rate\": 0.01,\n",
      "    \"num_warmup_steps\": 0,\n",
      "    \"num_train_steps\": 3780\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_files = [output_path / f'train_metrics_{idx}.csv' for idx in range(n_splits)]\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "\n",
    "best_model_index = None\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    val_accuracy = df.iloc[-1]['val_accuracy']\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_index = i\n",
    "\n",
    "print(f\"Best model index: {best_model_index}, val_accuracy: {best_val_accuracy}\")\n",
    "\n",
    "\n",
    "best_model = models_with_histories[best_model_index].get('model', None)\n",
    "best_model.save_pretrained(output_path / 'best_model', from_tf=True) \n",
    "\n",
    "best_model_info = {\"idx\": best_model_index,\n",
    "                   \"model_id\": model_id,\n",
    "                   \"zoom\": zoom,\n",
    "                   \"n_splits\": n_splits,\n",
    "                   \"num_train_epochs\": num_train_epochs,\n",
    "                   \"train_batch_size\": train_batch_size,\n",
    "                   \"eval_batch_size\": eval_batch_size,\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"weight_decay_rate\": weight_decay_rate,\n",
    "                   \"num_warmup_steps\": num_warmup_steps,\n",
    "                   \"num_train_steps\": num_train_steps_list[best_model_index]}\n",
    "\n",
    "with open(output_path / 'best_model_info.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=4)\n",
    "\n",
    "print(json.dumps(best_model_info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": {\n",
      "        \"mean\": 0.9964607238769532,\n",
      "        \"std\": 0.006334462037977405\n",
      "    },\n",
      "    \"auc\": {\n",
      "        \"mean\": 0.9995569825172425,\n",
      "        \"std\": 0.0009748233614117515\n",
      "    },\n",
      "    \"auc_multi\": {\n",
      "        \"mean\": 0.9994521260261535,\n",
      "        \"std\": 0.0012057299075043642\n",
      "    },\n",
      "    \"loss\": {\n",
      "        \"mean\": 0.03335511106997724,\n",
      "        \"std\": 0.02062681463849152\n",
      "    },\n",
      "    \"precision\": {\n",
      "        \"mean\": 0.9974926948547364,\n",
      "        \"std\": 0.004672305560359958\n",
      "    },\n",
      "    \"recall\": {\n",
      "        \"mean\": 0.9954108953475952,\n",
      "        \"std\": 0.008055130940363905\n",
      "    },\n",
      "    \"val_accuracy\": {\n",
      "        \"mean\": 0.8717103123664856,\n",
      "        \"std\": 0.04753376068092139\n",
      "    },\n",
      "    \"val_auc\": {\n",
      "        \"mean\": 0.9222186923027038,\n",
      "        \"std\": 0.048153249907271344\n",
      "    },\n",
      "    \"val_auc_multi\": {\n",
      "        \"mean\": 0.8959536790847779,\n",
      "        \"std\": 0.06169958395199498\n",
      "    },\n",
      "    \"val_loss\": {\n",
      "        \"mean\": 0.44469166100025176,\n",
      "        \"std\": 0.17439586349469957\n",
      "    },\n",
      "    \"val_precision\": {\n",
      "        \"mean\": 0.8753076910972595,\n",
      "        \"std\": 0.046296832208950534\n",
      "    },\n",
      "    \"val_recall\": {\n",
      "        \"mean\": 0.8667141199111938,\n",
      "        \"std\": 0.05027077812689226\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = ['accuracy', 'auc', 'auc_multi', 'loss', 'precision', 'recall',\n",
    "                   'val_accuracy', 'val_auc', 'val_auc_multi', 'val_loss', 'val_precision', 'val_recall']\n",
    "\n",
    "last_rows_numeric = [df[numeric_columns].iloc[-1] for df in dataframes]\n",
    "mean_metrics = pd.concat(last_rows_numeric, axis=1).mean(axis=1)\n",
    "std_metrics = pd.concat(last_rows_numeric, axis=1).std(axis=1)\n",
    "\n",
    "metrics_dict = {\n",
    "    metric_name: {\n",
    "        \"mean\": mean_metrics[metric_name],\n",
    "        \"std\": std_metrics[metric_name],\n",
    "    }\n",
    "    for metric_name in mean_metrics.index\n",
    "}\n",
    "\n",
    "with open(output_path / 'train_metrics_mean_with_std.json', 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(json.dumps(metrics_dict, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
