{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreakHis Image Classification with ðŸ¤— Vision Transformers and `TensorFlow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick intro: Vision Transformer (ViT) by Google Brain\n",
    "The Vision Transformer (ViT) is basically BERT, but applied to images. It attains excellent results compared to state-of-the-art convolutional networks. In order to provide images to the model, each image is split into a sequence of fixed-size patches (typically of resolution 16x16 or 32x32), which are linearly embedded. One also adds a [CLS] token at the beginning of the sequence in order to classify images. Next, one adds absolute position embeddings and provides this sequence to the Transformer encoder.\n",
    "\n",
    "* [Original paper](https://arxiv.org/abs/2010.11929)\n",
    "* [Official repo (in JAX)](https://github.com/google-research/vision_transformer)\n",
    "* [ðŸ¤— Vision Transformer](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "* [Pre-trained model](https://huggingface.co/google/vit-base-patch16-224-in21k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets \"tensorflow==2.6.0\" tensorflow-addons --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step, we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning process, e.g. `feature extractor` and `model` we will use. \n",
    "\n",
    "In this example we are going to fine-tune the [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) a Vision Transformer (ViT) pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.\n",
    "There are also [large](https://huggingface.co/google/vit-large-patch16-224-in21k) and [huge](https://huggingface.co/google/vit-huge-patch14-224-in21k) flavors of original ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "zoom = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import create_optimizer, DefaultDataCollator, ViTImageProcessor, TFViTForImageClassification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Pre-processing\n",
    "\n",
    "- **Data Source:** https://www.kaggle.com/code/nasrulhakim86/breast-cancer-histopathology-images-classification/data\n",
    "- The Breast Cancer Histopathological Image Classification (BreakHis) is composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients.\n",
    "- The images are collected using different magnifying factors (40X, 100X, 200X, and 400X). \n",
    "- To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format).\n",
    "- This database has been built in collaboration with the P&D Laboratory â€“ Pathological Anatomy and Cytopathology, Parana, Brazil (http://www.prevencaoediagnose.com.br). \n",
    "- Each image filename stores information about the image itself: method of procedure biopsy, tumor class, tumor type, patient identification, and magnification factor. \n",
    "- For example, SOBBTA-14-4659-40-001.png is the image 1, at magnification factor 40X, of a benign tumor of type tubular adenoma, original from the slide 14-4659, which was collected by procedure SOB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BreakHis` is not yet available as a dataset in the `datasets` library. To be able to create a `Dataset` instance we need to write a small little helper function, which will load our `Dataset` from the filesystem and create the instance to use later for training.\n",
    "\n",
    "This notebook assumes that the dataset is available in directory tree next to this file and its directory name is `breakhis_400x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "cwd = Path().absolute()\n",
    "input_path = cwd / f'breakhis_{zoom}x'\n",
    "output_path = cwd / 'results' / f'{zoom}x_{now}'\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_path, ignore_errors=True)\n",
    "os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.disable_traceback_filtering()\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_id)\n",
    "\n",
    "def process_example(image):\n",
    "    inputs = image_processor(image, return_tensors='tf')\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "\n",
    "def process_dataset(example):\n",
    "    example['pixel_values'] = process_example(Image.open(example['file_loc']).convert(\"RGB\"))\n",
    "\n",
    "    example['label'] = to_categorical(example['label'], num_classes=2)\n",
    "    return example\n",
    "\n",
    "def load_data(idx):\n",
    "    train_csv = str(input_path / f\"train_{idx}.csv\")\n",
    "    val_csv = str(input_path / f\"val_{idx}.csv\")\n",
    "    dataset = load_dataset(\n",
    "        'csv', data_files={'train': train_csv, 'val': val_csv})\n",
    "\n",
    "    dataset = dataset.map(process_dataset, with_indices=False, num_proc=1)\n",
    "\n",
    "    print(f\"Loaded {idx} dataset: {dataset}\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model using `Keras`\n",
    "\n",
    "Now that our `dataset` is processed, we can download the pretrained model and fine-tune it. But before we can do this we need to convert our Hugging Face `datasets` Dataset into a `tf.data.Dataset`. For this, we will use the `.to_tf_dataset` method and a `data collator` (Data collators are objects that will form a batch by using a list of dataset elements as input).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\"0\": \"benign\", \"1\": \"malignant\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_train_epochs = 10\n",
    "train_batch_size = 10\n",
    "eval_batch_size = 10\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "output_dir = model_id.split(\"/\")[1]\n",
    "fp16 = True\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "if fp16:\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained transformer model and fine-tune it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, val, idx):\n",
    "    # define loss\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # define metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.AUC(name='auc', from_logits=True),\n",
    "        tf.keras.metrics.AUC(name='auc_multi', from_logits=True, num_labels=2, multi_label=True),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5),\n",
    "    ]\n",
    "    \n",
    "    num_train_steps = len(train) * num_train_epochs\n",
    "    print(f\"num_train_steps = {num_train_steps}\")\n",
    "    optimizer, _ = create_optimizer(\n",
    "        init_lr=learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        weight_decay_rate=weight_decay_rate,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "    )\n",
    "\n",
    "    # load pre-trained ViT model\n",
    "    model = TFViTForImageClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_accuracy\", patience=3),\n",
    "        CSVLogger(output_path / f'train_metrics_{idx}.csv')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        callbacks=callbacks,\n",
    "        epochs=num_train_epochs,\n",
    "    )\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_dim(example):\n",
    "    example['pixel_values'] = np.squeeze(example['pixel_values'], axis=0)\n",
    "    return example\n",
    "\n",
    "def save_model(idx, model):\n",
    "    model.save_pretrained(output_path / f'model_{idx}', from_tf=True)\n",
    "    \n",
    "def save_history(idx, history):\n",
    "    np.save(output_path / f'train_history_{idx}.npy', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "def run_fold(idx):\n",
    "    tf.keras.backend.clear_session()\n",
    "    dataset = load_data(idx)\n",
    "\n",
    "    # Check patient ids uniqueness\n",
    "    train_dataset = dataset[\"train\"].map(remove_extra_dim)\n",
    "    val_dataset = dataset[\"val\"].map(remove_extra_dim)\n",
    "\n",
    "    # Create datasets and train model\n",
    "    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "    train_dataset_tf = train_dataset.to_tf_dataset(\n",
    "        columns=['pixel_values'],\n",
    "        label_cols=['label'],\n",
    "        shuffle=True,\n",
    "        batch_size=train_batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    val_dataset_tf = val_dataset.to_tf_dataset(\n",
    "        columns=['pixel_values'],\n",
    "        label_cols=['label'],\n",
    "        shuffle=True,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    print(train_dataset_tf)\n",
    "    print(val_dataset_tf)\n",
    "\n",
    "    model, history = train_model(train_dataset_tf, val_dataset_tf, idx)\n",
    "    save_model(idx, model)\n",
    "    save_history(idx, history)\n",
    "\n",
    "    print(f'Fold {idx} finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "for idx in range(n_splits):\n",
    "    run_fold(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('idx', type=int, help='Fold index')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     run_fold(args.idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process\n",
    "# # create a process\n",
    "# n_splits = 5\n",
    "# for idx in range(n_splits):\n",
    "#     training_process = Process(target=lambda: run_fold(idx))\n",
    "#     # run the process\n",
    "#     training_process.start()\n",
    "#     # wait for the process to finish\n",
    "#     print('Waiting for the process...')\n",
    "#     training_process.join()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
