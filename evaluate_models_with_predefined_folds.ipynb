{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreakHis Image Classification with ðŸ¤— Vision Transformers and `TensorFlow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets \"tensorflow==2.6.0\" tensorflow-addons --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step, we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning process, e.g. `feature extractor` and `model` we will use. \n",
    "\n",
    "In this example we are going to fine-tune the [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) a Vision Transformer (ViT) pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.\n",
    "There are also [large](https://huggingface.co/google/vit-large-patch16-224-in21k) and [huge](https://huggingface.co/google/vit-huge-patch14-224-in21k) flavors of original ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "n_splits = 5\n",
    "\n",
    "cwd = Path().absolute()\n",
    "results_path = cwd / 'results_100x'\n",
    "\n",
    "\n",
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import create_optimizer, DefaultDataCollator, ViTImageProcessor, TFViTForImageClassification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Pre-processing\n",
    "\n",
    "- **Data Source:** https://www.kaggle.com/code/nasrulhakim86/breast-cancer-histopathology-images-classification/data\n",
    "- The Breast Cancer Histopathological Image Classification (BreakHis) is composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients.\n",
    "- The images are collected using different magnifying factors (40X, 100X, 200X, and 400X). \n",
    "- To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format).\n",
    "- This database has been built in collaboration with the P&D Laboratory â€“ Pathological Anatomy and Cytopathology, Parana, Brazil (http://www.prevencaoediagnose.com.br). \n",
    "- Each image filename stores information about the image itself: method of procedure biopsy, tumor class, tumor type, patient identification, and magnification factor. \n",
    "- For example, SOBBTA-14-4659-40-001.png is the image 1, at magnification factor 40X, of a benign tumor of type tubular adenoma, original from the slide 14-4659, which was collected by procedure SOB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BreakHis` is not yet available as a dataset in the `datasets` library. To be able to create a `Dataset` instance we need to write a small little helper function, which will load our `Dataset` from the filesystem and create the instance to use later for training.\n",
    "\n",
    "This notebook assumes that the dataset is available in directory tree next to this file and its directory name is `breakhis_400x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_paths = [os.path.basename(f.path) for f in os.scandir(results_path) if f.is_dir()]\n",
    "\n",
    "output_paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_best_model_idx_and_acc(output_path):\n",
    "    csv_files = [results_path / output_path / f'train_metrics_{idx}.csv' for idx in range(n_splits)]\n",
    "    dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "\n",
    "    best_model_index = None\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        val_accuracy = df.iloc[-1]['val_accuracy']\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_index = i\n",
    "\n",
    "    print(f\"Best model index: {best_model_index}, val_accuracy: {best_val_accuracy}\")\n",
    "    return best_model_index, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "for output_path in output_paths:\n",
    "    best_models[output_path] = (find_best_model_idx_and_acc(output_path))\n",
    "\n",
    "print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(x):\n",
    "    try:\n",
    "        # Usuwamy nawiasy kwadratowe i dzielimy string na listÄ™, uÅ¼ywajÄ…c przecinka jako separatora\n",
    "        lst = json.loads(x)\n",
    "        lst = lst.replace(\"[\", \"\").replace(\"]\", \"\").split(\", \")\n",
    "        # Konwersja kaÅ¼dego elementu listy na float\n",
    "        lst = [float(i) for i in lst]\n",
    "        return np.mean(lst)\n",
    "    except ValueError as e:\n",
    "        print(f\"Cannot convert {x} to list of floats: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_mean_metrics(output_path):\n",
    "    numeric_columns = ['accuracy', 'auc', 'loss', 'precision', 'recall', 'f1_score',\n",
    "                    'val_accuracy', 'val_auc', 'val_loss', 'val_precision', 'val_recall', 'val_f1_score']\n",
    "\n",
    "    csv_files = [results_path / output_path / f'train_metrics_{idx}.csv' for idx in range(n_splits)]\n",
    "    dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "    \n",
    "    for df in dataframes:\n",
    "        df['f1_score'] = df['f1_score'].apply(compute_mean)\n",
    "        df['val_f1_score'] = df['val_f1_score'].apply(compute_mean)\n",
    "    \n",
    "    last_rows_numeric = [df[numeric_columns].iloc[-1] for df in dataframes]\n",
    "    mean_metrics = pd.concat(last_rows_numeric, axis=1).mean(axis=1)\n",
    "    std_metrics = pd.concat(last_rows_numeric, axis=1).std(axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        metric_name: {\n",
    "            \"mean\": mean_metrics[metric_name],\n",
    "            \"std\": std_metrics[metric_name],\n",
    "        }\n",
    "        for metric_name in mean_metrics.index\n",
    "    }\n",
    "\n",
    "    with open(results_path / output_path / 'train_metrics_mean_with_std.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    metrics[\"output_path\"] = output_path\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics = []\n",
    "for output_path in output_paths:\n",
    "    mean_metrics.append(calculate_mean_metrics(output_path))\n",
    "\n",
    "max_val_acc = max(mean_metrics, key=lambda x: x['val_accuracy']['mean'])\n",
    "max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mm in mean_metrics:\n",
    "    print(mm['output_path'], mm['val_accuracy']['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(results_path / f'model_info_{best_model_index}.json', 'r') as f:\n",
    "best_model_output_path = max_val_acc[\"output_path\"]\n",
    "best_model_index = best_models[best_model_output_path][0]\n",
    "with open(results_path / best_model_output_path / f'model_info_{best_model_index}.json', 'r') as f:\n",
    "    best_model_info = json.load(f)\n",
    "\n",
    "best_model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(best_model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = cwd / f'breakhis_{best_model_info[\"zoom\"]}x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_files = [results_path / best_model_output_path / f'train_metrics_{idx}.csv' for idx in range(best_model_info[\"n_splits\"])]\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in dataframes:\n",
    "    df['f1_score'] = df['f1_score'].apply(compute_mean)\n",
    "    df['val_f1_score'] = df['val_f1_score'].apply(compute_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy metryki dla treningu i walidacji osobno\n",
    "train_metrics = ['accuracy', 'loss', 'auc', 'precision', 'recall', 'f1_score']\n",
    "val_metrics = ['val_accuracy', 'val_loss', 'val_auc', 'val_precision', 'val_recall', 'val_f1_score']\n",
    "\n",
    "assert len(train_metrics) == len(val_metrics), \"Liczba metryk treningowych musi byÄ‡ taka sama jak liczba metryk walidacyjnych\"\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "num_metrics = len(train_metrics)\n",
    "\n",
    "# Modyfikujemy figsize, aby wykresy byÅ‚y bardziej kwadratowe\n",
    "fig, axes = plt.subplots(num_metrics, 2, figsize=(12, num_metrics * 3))\n",
    "\n",
    "title_postfix = f'(powiÄ™kszenie {best_model_info[\"zoom\"]}x)'\n",
    "train_title_midfix = 'na zbiorze treningowym'\n",
    "val_title_midfix = 'na zbiorze walidacyjnym' \n",
    "metrics_mapping = {'accuracy': 'SkutecznoÅ›Ä‡', 'auc': 'AUC', 'loss': 'Strata', 'precision': 'Precyzja', 'recall': 'CzuÅ‚oÅ›Ä‡', 'f1_score': 'F1-score',\n",
    "                   'val_accuracy': 'SkutecznoÅ›Ä‡', 'val_auc': 'AUC', 'val_loss': 'Strata', 'val_precision': 'Precyzja', 'val_recall': 'CzuÅ‚oÅ›Ä‡', 'val_f1_score': 'F1-score'}\n",
    "\n",
    "for i, (train_metric, val_metric) in enumerate(zip(train_metrics, val_metrics)):\n",
    "    for j, df in enumerate(dataframes):\n",
    "        axes[i, 0].plot(df[train_metric], label=f'Fold {j}')\n",
    "        axes[i, 1].plot(df[val_metric], label=f'Fold {j}')\n",
    "\n",
    "    train_title = f'{metrics_mapping[train_metric]} {train_title_midfix} {title_postfix}'\n",
    "    val_title = f'{metrics_mapping[val_metric]} {val_title_midfix} {title_postfix}'\n",
    "\n",
    "    axes[i, 0].set_title(train_title)\n",
    "    axes[i, 0].set_ylabel(metrics_mapping[train_metric])\n",
    "    axes[i, 1].set_xlabel('Epoka')\n",
    "    axes[i, 0].legend()\n",
    "\n",
    "    axes[i, 1].set_title(val_title)\n",
    "    axes[i, 1].set_ylabel(metrics_mapping[val_metric])\n",
    "    axes[i, 1].set_xlabel('Epoka')\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path / best_model_output_path / f'train_metrics_{best_model_info[\"zoom\"]}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final evaluation on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.disable_traceback_filtering()\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(best_model_info['model_id'])\n",
    "\n",
    "\n",
    "def remove_extra_dim(example):\n",
    "    example['pixel_values'] = np.squeeze(example['pixel_values'], axis=0)\n",
    "    return example\n",
    "\n",
    "\n",
    "def process_example(image):\n",
    "    inputs = image_processor(image, return_tensors='tf')\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "\n",
    "def process_dataset(example):\n",
    "    example['pixel_values'] = process_example(\n",
    "        Image.open(example['file_loc']).convert(\"RGB\"))\n",
    "\n",
    "    # example['pixel_values']=np.squeeze(example['pixel_values'], axis=0)\n",
    "    example['label'] = to_categorical(example['label'], num_classes=2)\n",
    "    return example\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test_csv = str(input_path / 'test.csv')\n",
    "    dataset = load_dataset(\n",
    "        'csv', data_files={'test': test_csv})\n",
    "\n",
    "    dataset = dataset.map(process_dataset, with_indices=False, num_proc=1)\n",
    "\n",
    "    print(f\"Loaded test dataset: {len(dataset['test'])} samples\")\n",
    "\n",
    "    return dataset.map(remove_extra_dim)\n",
    "\n",
    "\n",
    "test_dataset = load_test_data()\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "test_dataset_tf = test_dataset['test'].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=['label'],\n",
    "    shuffle=False,\n",
    "    batch_size=best_model_info['batch_size'],\n",
    "    collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TFViTForImageClassification.from_pretrained(results_path / best_model_output_path / f'model_{best_model_index}')\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.AUC(name='auc', from_logits=True),\n",
    "    tf.keras.metrics.AUC(name='auc_multi', from_logits=True, num_labels=2, multi_label=True),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=best_model_info['learning_rate'],\n",
    "    num_train_steps=best_model_info['num_train_steps'],\n",
    "    weight_decay_rate=best_model_info['weight_decay_rate'],\n",
    "    num_warmup_steps=best_model_info['num_warmup_steps'],\n",
    ")\n",
    "best_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "results = best_model.evaluate(test_dataset_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset evaluation results:\")\n",
    "test_metrics = {}\n",
    "for metric, value in zip(best_model.metrics_names, results):\n",
    "    print(metric, value)\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = list(value)\n",
    "        value = [str(v) for v in value]\n",
    "    else:\n",
    "        value = str(value)\n",
    "    test_metrics[metric] = value\n",
    "\n",
    "with open(results_path / best_model_output_path / 'test_metrics.json', 'w') as test_metrics_json:\n",
    "    json.dump(test_metrics, test_metrics_json, indent = 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the details of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(test_dataset_tf)\n",
    "probabilities = tf.nn.softmax(preds.logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = np.argmax(probabilities, axis=-1)\n",
    "labels_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_dataset['test']['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract file locations and real labels from the test dataset\n",
    "file_locs = [example['file_loc'] for example in test_dataset['test']]\n",
    "labels = [np.argmax(example['label']) for example in test_dataset['test']]\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(\n",
    "    {'file_loc': file_locs, 'label': labels, 'label_pred': labels_pred})\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "results_df.to_csv(results_path / best_model_output_path / f'test_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
