{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BreakHis Image Classification with ðŸ¤— Vision Transformers and `TensorFlow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step, we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning process, e.g. `feature extractor` and `model` we will use. \n",
    "\n",
    "In this example we are going to fine-tune the [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) a Vision Transformer (ViT) pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.\n",
    "There are also [large](https://huggingface.co/google/vit-large-patch16-224-in21k) and [huge](https://huggingface.co/google/vit-huge-patch14-224-in21k) flavors of original ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/miki/repos/uz/breakhis/vcs/results/400x_ConvNext_PT_patches224')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "n_splits = 5\n",
    "\n",
    "cwd = Path().absolute()\n",
    "results_path = cwd / 'results' / '400x_ConvNext_PT_patches224'\n",
    "\n",
    "\n",
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miki/miniconda3/envs/pt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "# from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "from transformers import create_optimizer, DefaultDataCollator, AutoImageProcessor, AutoModelForImageClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_0', 'model_1', 'model_2', 'model_3', 'model_4']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_paths = sorted([os.path.basename(f.path) for f in os.scandir(results_path) if f.is_dir()])\n",
    "\n",
    "output_paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_best_model_idx_and_acc(output_paths):\n",
    "    json_files = [results_path / output_path / 'all_results.json' for output_path in output_paths]\n",
    "    # csv_files = [results_path / output_path / f'train_metrics_{idx}.csv' for idx in range(n_splits)]\n",
    "    # csv_files = [results_path / f'train_metrics_{idx}.csv' for idx in range(n_splits)]\n",
    "    # dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "\n",
    "    best_model_index = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    for i, json_files in enumerate(json_files):\n",
    "        with open(json_files) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            print(f\"Model {i}: e_acc: {data['eval_accuracy']:.3f}, e_loss: {data['eval_loss']:.3f}, e_f1: {data['eval_f1']:.3f}\")\n",
    "            f1_score = data[\"eval_f1\"]\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_index = i\n",
    "\n",
    "    print(f\"Best model index: {best_model_index}, f1_score: {best_f1_score}\")\n",
    "    return best_model_index, best_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: e_acc: 0.783, e_loss: 0.546, e_f1: 0.848\n",
      "Model 1: e_acc: 0.742, e_loss: 1.186, e_f1: 0.841\n",
      "Model 2: e_acc: 0.895, e_loss: 0.524, e_f1: 0.927\n",
      "Model 3: e_acc: 0.808, e_loss: 0.891, e_f1: 0.841\n",
      "Model 4: e_acc: 0.845, e_loss: 0.508, e_f1: 0.878\n",
      "Best model index: 2, f1_score: 0.9266187050359712\n"
     ]
    }
   ],
   "source": [
    "best_model = (find_best_model_idx_and_acc(output_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_metrics():\n",
    "    numeric_columns = ['eval_accuracy', 'eval_auc', 'eval_f1', 'eval_loss', 'eval_precision', 'eval_recall', 'train_loss']\n",
    "    n_splits = 5  # Define the number of splits you have\n",
    "\n",
    "    json_files = [results_path / f'model_{idx}' / 'all_results.json' for idx in range(n_splits)]\n",
    "    \n",
    "    data_list = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "            # append dict to the list, but only `numeric_columns`\n",
    "            data_list.append({k: v for k, v in data.items() if k in numeric_columns})\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    mean_metrics = df.mean()\n",
    "    std_metrics = df.std()\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    metrics = {\n",
    "        metric_name: {\n",
    "            \"mean\": mean_metrics[metric_name],\n",
    "            \"std\": std_metrics[metric_name],\n",
    "        }\n",
    "        for metric_name in mean_metrics.index\n",
    "    }\n",
    "\n",
    "    with open(results_path / 'train_metrics_mean_with_std.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    metrics[\"output_path\"] = str(results_path / 'train_metrics_mean_with_std.json')\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_accuracy  eval_auc   eval_f1  eval_loss  eval_precision  eval_recall  \\\n",
      "0       0.783499  0.868032  0.848391   0.545576        0.825660     0.872408   \n",
      "1       0.742038  0.765488  0.841280   1.185728        0.931983     0.766667   \n",
      "2       0.894737  0.874649  0.926619   0.523717        0.864430     0.998450   \n",
      "3       0.808282  0.816104  0.841438   0.891135        0.845370     0.837542   \n",
      "4       0.845238  0.908499  0.877909   0.507963        0.895087     0.861378   \n",
      "\n",
      "   train_loss  \n",
      "0    0.103908  \n",
      "1    0.091258  \n",
      "2    0.101322  \n",
      "3    0.088616  \n",
      "4    0.092490  \n"
     ]
    }
   ],
   "source": [
    "# mean_metrics = []\n",
    "mean_metrics = calculate_mean_metrics()\n",
    "\n",
    "# max_val_acc = max(mean_metrics, key=lambda x: x['val_accuracy']['mean'])\n",
    "# max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mm in mean_metrics:\n",
    "    print(mm['output_path'], mm['val_accuracy']['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(results_path / f'model_info_{best_model_index}.json', 'r') as f:\n",
    "best_model_output_path = max_val_acc[\"output_path\"]\n",
    "best_model_index = best_models[best_model_output_path][0]\n",
    "with open(results_path / best_model_output_path / f'model_info_{best_model_index}.json', 'r') as f:\n",
    "    best_model_info = json.load(f)\n",
    "\n",
    "best_model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(best_model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = cwd / f'breakhis_{best_model_info[\"zoom\"]}x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_files = [results_path / best_model_output_path / f'train_metrics_{idx}.csv' for idx in range(best_model_info[\"n_splits\"])]\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in dataframes:\n",
    "    df['f1_score'] = df['f1_score'].apply(compute_mean)\n",
    "    df['val_f1_score'] = df['val_f1_score'].apply(compute_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = ['accuracy', 'loss', 'auc', 'precision', 'recall', 'f1_score']\n",
    "val_metrics = ['val_accuracy', 'val_loss', 'val_auc', 'val_precision', 'val_recall', 'val_f1_score']\n",
    "\n",
    "assert len(train_metrics) == len(val_metrics), \"Number of train and validation metrics must be equal!\"\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "num_metrics = len(train_metrics)\n",
    "\n",
    "fig, axes = plt.subplots(num_metrics, 2, figsize=(12, num_metrics * 3))\n",
    "\n",
    "title_postfix = f'(Zoom {best_model_info[\"zoom\"]}x)'\n",
    "train_title_midfix = 'on training set'\n",
    "val_title_midfix = 'on validation set' \n",
    "metrics_mapping = {'accuracy': 'Accuracy', 'auc': 'AUC', 'loss': 'Loss', 'precision': 'Precision', 'recall': 'Recall', 'f1_score': 'F1-score',\n",
    "                   'val_accuracy': 'Accuracy', 'val_auc': 'AUC', 'val_loss': 'Loss', 'val_precision': 'Precision', 'val_recall': 'Recall', 'val_f1_score': 'F1-score'}\n",
    "\n",
    "for i, (train_metric, val_metric) in enumerate(zip(train_metrics, val_metrics)):\n",
    "    for j, df in enumerate(dataframes):\n",
    "        axes[i, 0].plot(df[train_metric], label=f'Fold {j}')\n",
    "        axes[i, 1].plot(df[val_metric], label=f'Fold {j}')\n",
    "\n",
    "    train_title = f'{metrics_mapping[train_metric]} {train_title_midfix} {title_postfix}'\n",
    "    val_title = f'{metrics_mapping[val_metric]} {val_title_midfix} {title_postfix}'\n",
    "\n",
    "    axes[i, 0].set_title(train_title)\n",
    "    axes[i, 0].set_ylabel(metrics_mapping[train_metric])\n",
    "    axes[i, 1].set_xlabel('Epoka')\n",
    "    axes[i, 0].legend()\n",
    "\n",
    "    axes[i, 1].set_title(val_title)\n",
    "    axes[i, 1].set_ylabel(metrics_mapping[val_metric])\n",
    "    axes[i, 1].set_xlabel('Epoka')\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path / best_model_output_path / f'train_metrics_{best_model_info[\"zoom\"]}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final evaluation on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.disable_traceback_filtering()\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(best_model_info['model_id'])\n",
    "\n",
    "\n",
    "def remove_extra_dim(example):\n",
    "    example['pixel_values'] = np.squeeze(example['pixel_values'], axis=0)\n",
    "    return example\n",
    "\n",
    "\n",
    "def process_example(image):\n",
    "    inputs = image_processor(image, return_tensors='tf')\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "\n",
    "def process_dataset(example):\n",
    "    example['pixel_values'] = process_example(\n",
    "        Image.open(example['file_loc']).convert(\"RGB\"))\n",
    "\n",
    "    # example['pixel_values']=np.squeeze(example['pixel_values'], axis=0)\n",
    "    example['label'] = to_categorical(example['label'], num_classes=2)\n",
    "    return example\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    test_csv = str(input_path / 'test.csv')\n",
    "    dataset = load_dataset(\n",
    "        'csv', data_files={'test': test_csv})\n",
    "\n",
    "    dataset = dataset.map(process_dataset, with_indices=False, num_proc=1)\n",
    "\n",
    "    print(f\"Loaded test dataset: {len(dataset['test'])} samples\")\n",
    "\n",
    "    return dataset.map(remove_extra_dim)\n",
    "\n",
    "\n",
    "test_dataset = load_test_data()\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "test_dataset_tf = test_dataset['test'].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=['label'],\n",
    "    shuffle=False,\n",
    "    batch_size=best_model_info['batch_size'],\n",
    "    collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TFViTForImageClassification.from_pretrained(results_path / best_model_output_path / f'model_{best_model_index}')\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.AUC(name='auc', from_logits=True),\n",
    "    tf.keras.metrics.AUC(name='auc_multi', from_logits=True, num_labels=2, multi_label=True),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=best_model_info['learning_rate'],\n",
    "    num_train_steps=best_model_info['num_train_steps'],\n",
    "    weight_decay_rate=best_model_info['weight_decay_rate'],\n",
    "    num_warmup_steps=best_model_info['num_warmup_steps'],\n",
    ")\n",
    "best_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "results = best_model.evaluate(test_dataset_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset evaluation results:\")\n",
    "test_metrics = {}\n",
    "for metric, value in zip(best_model.metrics_names, results):\n",
    "    print(metric, value)\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = list(value)\n",
    "        value = [str(v) for v in value]\n",
    "    else:\n",
    "        value = str(value)\n",
    "    test_metrics[metric] = value\n",
    "\n",
    "with open(results_path / best_model_output_path / 'test_metrics.json', 'w') as test_metrics_json:\n",
    "    json.dump(test_metrics, test_metrics_json, indent = 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the details of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(test_dataset_tf)\n",
    "probabilities = tf.nn.softmax(preds.logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = np.argmax(probabilities, axis=-1)\n",
    "labels_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_dataset['test']['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract file locations and real labels from the test dataset\n",
    "file_locs = [example['file_loc'] for example in test_dataset['test']]\n",
    "labels = [np.argmax(example['label']) for example in test_dataset['test']]\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(\n",
    "    {'file_loc': file_locs, 'label': labels, 'label_pred': labels_pred})\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "results_df.to_csv(results_path / best_model_output_path / f'test_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
